{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from get_data import get_all_memes, save_meme_tags, get_memedroid_data, get_twitter_data, get_imgur_data, get_reddit_data\n",
    "from utils import train_model, get_popular_tag, get_source_upvotes, upload_blob, upload_to_bucket\n",
    "from pyspark.ml import PipelineModel\n",
    "from google.cloud import storage\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 10\n",
    "\n",
    "# memes_df = get_all_memes(spark)\n",
    "# memes_df = memes_df.cache()\n",
    "# memes_df_train = memes_df.drop_duplicates(subset=['text'])\n",
    "# model = train_model(memes_df_train, number_of_clusters, 50)\n",
    "\n",
    "model = PipelineModel.load(\"hdfs:///models/model\")\n",
    "X = model.transform(memes_df)\n",
    "X = X.select('id', 'url', 'image_path', 'source', 'prediction','additional_data')\n",
    "X = X.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1| 2844|\n",
      "|         6|  709|\n",
      "|         3|30751|\n",
      "|         5| 1093|\n",
      "|         9| 5886|\n",
      "|         4|38346|\n",
      "|         8| 6791|\n",
      "|         7| 8018|\n",
      "|         2| 6742|\n",
      "|         0| 8232|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_tag(list_of_tags, cluster_names):\n",
    "    \"\"\"From list_of_tags return the most popular, unique tag\"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    tags_to_omit = ['', 'memes', 'meme', 'funny','dump', 'memedump', 'dankmemes', 'reddit', 'description', 'twitter', 'lol', 'thumbnailhash']\n",
    "    while True:\n",
    "        if list_of_tags[i][0] not in tags_to_omit and list_of_tags[i][0] not in cluster_names:\n",
    "            if list_of_tags[i][0] == 'randommemedump':\n",
    "                list_of_tags[i] = list(list_of_tags[i])\n",
    "                list_of_tags[i][0] = 'other'\n",
    "                list_of_tags = tuple(list_of_tags)\n",
    "            return list_of_tags[i][0]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meme_tags(X, number_of_clusters)\n",
    "\n",
    "cluster_names = []\n",
    "for cluster_id in range(number_of_clusters):\n",
    "    words = sc.textFile(\"hdfs:///tags/all_tags_{0}\".format(cluster_id)).flatMap(lambda line: line.split(\" \"))\n",
    "    wordCounts = words.map(lambda word: (re.sub('[^A-Za-z0-9]+', '', word.lower()), 1)).reduceByKey(lambda a,b:a +b)\n",
    "    tags = wordCounts.sortBy(lambda atuple: atuple[1], ascending = False).collect()\n",
    "    cluster_names.append(get_popular_tag(tags, cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome',\n",
       " 'youtube',\n",
       " 'wholesome',\n",
       " 'other',\n",
       " 'humor',\n",
       " 'tiktok',\n",
       " 'asua4r3',\n",
       " 'gaming',\n",
       " 'history',\n",
       " 'coronavirus']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.createDataFrame([(i, cluster_names[i]) for i in range(number_of_clusters)], [\"prediction\", \"cluster\"])\n",
    "\n",
    "memedroid = get_memedroid_data(memes_df)\n",
    "memedroid = get_source_upvotes(memedroid, 'upvotes', 'memedroid')\n",
    "memedroid.cache()\n",
    "twitter = get_twitter_data(memes_df)\n",
    "twitter = get_source_upvotes(twitter, 'favorite_count', 'twitter')\n",
    "twitter.cache()\n",
    "reddit = get_reddit_data(memes_df)\n",
    "reddit = get_source_upvotes(reddit, 'upvotes', 'reddit')\n",
    "reddit.cache()\n",
    "imgur = get_imgur_data(memes_df)\n",
    "imgur = get_source_upvotes(imgur, 'ups', 'imgur')\n",
    "imgur.cache()\n",
    "\n",
    "all_upvotes = memedroid.union(twitter).union(reddit).union(imgur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.join(all_upvotes, on = ['id'], how=\"left_outer\")\n",
    "\n",
    "X = X.join(tmp, on = ['prediction'], how=\"left_outer\")\n",
    "\n",
    "X = X.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prediction: integer (nullable = false)\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- additional_data: string (nullable = true)\n",
      " |-- upvotes: integer (nullable = true)\n",
      " |-- upvotes_centile: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/data_to_upload_on_bucket/tmp.json uploaded to tmp-data-for-website/all_data.json.\n"
     ]
    }
   ],
   "source": [
    "#if all to be uploaded\n",
    "X = X.select('id', 'url', 'image_path', 'source', 'timestamp', 'upvotes', 'upvotes_centile', 'cluster')\n",
    "\n",
    "X.coalesce(1).write.format('json').mode(\"overwrite\").save('hdfs:///json_bucket')\n",
    "\n",
    "\n",
    "upload_to_bucket('/json_bucket',  \"tmp-data-for-website/all_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/data_to_upload_on_bucket/tmp.json uploaded to tmp-data-for-website/daily/daily.json.\n"
     ]
    }
   ],
   "source": [
    "Y = X.select('id', 'upvotes', 'upvotes_centile', 'cluster')\n",
    "\n",
    "Y.coalesce(1).write.format('json').mode(\"overwrite\").save('hdfs:///json_bucket')\n",
    "\n",
    "\n",
    "upload_to_bucket('/json_bucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.sql.functions import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.transform(memes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol='features')\n",
    "silhouette = evaluator.evaluate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.__dict__['stages'][4].clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model_pca = pca.fit(X)\n",
    "X = model_pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_pca = [None]*len(centers)\n",
    "for i in range(len(centers)):\n",
    "    centers_pca[i] = np.multiply(model_pca.pc.toArray().T, centers[i]).sum(axis = 1)\n",
    "centers_pca = np.array(centers_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_df = X\n",
    "\n",
    "split_col = functions.split(memes_df[\"pcaFeatures\"].cast(StringType()), ',')\n",
    "memes_df = memes_df.withColumn('x', translate(split_col.getItem(0), \"[\", \"\").cast(DoubleType()))\n",
    "memes_df = memes_df.withColumn('y', translate(split_col.getItem(1), \"]\", \"\").cast(DoubleType()))\n",
    "\n",
    "df = memes_df.toPandas()\n",
    "groups = df.groupby('prediction')\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05)\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5, label=name)\n",
    "    ax.text(centers_pca[name,0], centers_pca[name,1], s = name, fontsize = 10)\n",
    "ax.legend(loc='lower right', prop={'size': 9})\n",
    "ax.title.set_text(\"k={0}, wn={1}, Silhouette={2}\".format(10,50,silhouette))\n",
    "plt.show()\n",
    "print(\"PCA, explained variance= {0}\".format(model_pca.explainedVariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('clusterization_w2v2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
